import os
import tempfile
from typing import List, Tuple

import gradio as gr

from modules.data_classes import ModelConfig, WhisperParams
from modules.download_manager import get_download_manager
from modules.model_factory import (
    ModelFactory,
    get_available_implementations,
)


class VoiceCaptionUI:
    """è¯­éŸ³å­—å¹•æå–ç•Œé¢"""

    def __init__(self):
        self.current_pipeline = None
        self.download_manager = get_download_manager()
        self.available_implementations = get_available_implementations()

    def create_interface(self):
        """åˆ›å»º Gradio ç•Œé¢"""
        with gr.Blocks(title="è¯­éŸ³å­—å¹•æå–å™¨") as demo:
            gr.Markdown("# ğŸµ è¯­éŸ³å­—å¹•æå–å™¨")
            gr.Markdown("æ”¯æŒå¤šç§ Whisper å®ç°ï¼Œæ™ºèƒ½æ¨¡å‹ç®¡ç†ï¼Œè‡ªåŠ¨ä¸‹è½½åŠŸèƒ½")

            # ä¸»è¦è½¬å½•åŠŸèƒ½
            self.setup_transcription_interface()

            # ç³»ç»Ÿä¿¡æ¯ï¼ˆä½¿ç”¨ Accordionï¼Œé»˜è®¤å…³é—­ï¼‰
            with gr.Accordion("ğŸ“Š ç³»ç»Ÿä¿¡æ¯", open=False):
                self.render_system_info()

        return demo

    def setup_transcription_interface(self):
        """åˆ›å»ºè½¬å½•æ ‡ç­¾é¡µ"""
        # é…ç½®åŒºåŸŸ
        gr.Markdown("### âš™ï¸ é…ç½®é€‰é¡¹")

        with gr.Row():
            # Whisper å®ç°é€‰æ‹©
            whisper_type = gr.Dropdown(
                choices=list(self.available_implementations.keys()),
                value=ModelFactory.get_recommended_type(),
                label="ğŸ¤– Whisper å®ç°",
                info="é€‰æ‹© Whisper å®ç°ç±»å‹",
            )

        # æ¨¡å‹é€‰æ‹©ï¼ˆæ™ºèƒ½ä¸‹è½½ï¼‰
        recommended_type = ModelFactory.get_recommended_type()
        available_models = self.get_available_models(recommended_type)
        # ä¼˜å…ˆé€‰æ‹© medium æ¨¡å‹ï¼Œå¦‚æœä¸å¯ç”¨åˆ™é€‰æ‹©ç¬¬ä¸€ä¸ª
        default_model = None
        if available_models:
            if "medium" in available_models:
                default_model = "medium"
            else:
                default_model = available_models[0]

            model_size = gr.Dropdown(
                choices=available_models,
                value=default_model,
                label="ğŸ“¦ æ¨¡å‹å¤§å°",
                info="é€‰æ‹©æ¨¡å‹å¤§å°ï¼Œæœªä¸‹è½½çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸‹è½½",
                interactive=True,
            )

        # æ¨¡å‹ä¸‹è½½æŒ‰é’®ï¼ˆåˆå§‹åŒ–æ—¶æ£€æŸ¥çŠ¶æ€ï¼‰
        initial_whisper_type = ModelFactory.get_recommended_type()
        initial_models = self.get_available_models(initial_whisper_type)
        # ä¼˜å…ˆé€‰æ‹© medium æ¨¡å‹ï¼Œå¦‚æœä¸å¯ç”¨åˆ™é€‰æ‹©ç¬¬ä¸€ä¸ª
        initial_model = None
        if initial_models:
            if "medium" in initial_models:
                initial_model = "medium"
            else:
                initial_model = initial_models[0]

        # æ£€æŸ¥åˆå§‹æ¨¡å‹çŠ¶æ€
        if initial_model:
            _, initial_btn_state = self.check_model_download_status(
                initial_whisper_type, initial_model
            )
            download_model_btn = initial_btn_state
        else:
            download_model_btn = gr.Button(
                "ğŸ“¥ ä¸‹è½½æ¨¡å‹", variant="primary", visible=False
            )

        # éŸ³é¢‘ä¸Šä¼ åŒºåŸŸ
        gr.Markdown("### ğŸµ éŸ³é¢‘ä¸Šä¼ ")
        audio_input = gr.Audio(
            label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", type="filepath", sources=["upload"]
        )

        # é«˜çº§å‚æ•°åŒºåŸŸ
        with gr.Accordion("âš™ï¸ é«˜çº§å‚æ•°", open=False):
            with gr.Row():
                is_translate = gr.Checkbox(
                    label="ğŸ”„ ç¿»è¯‘ä¸ºè‹±æ–‡",
                    value=False,
                    info="æ˜¯å¦å°†ç»“æœç¿»è¯‘ä¸ºè‹±æ–‡",
                )

            with gr.Row():
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.0,
                    step=0.1,
                    label="ğŸŒ¡ï¸ æ¸©åº¦",
                    info="é‡‡æ ·æ¸©åº¦ï¼Œ0ä¸ºç¡®å®šæ€§è¾“å‡º",
                )

                beam_size = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=5,
                    step=1,
                    label="ğŸ” æŸæœç´¢å¤§å°",
                    info="æŸæœç´¢å¤§å°ï¼Œè¶Šå¤§è´¨é‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢",
                )

        # è½¬å½•æŒ‰é’®
        transcribe_btn = gr.Button("ğŸš€ å¼€å§‹è½¬å½•", variant="primary", size="lg")

        # ç»“æœæ˜¾ç¤ºåŒºåŸŸ
        gr.Markdown("### ğŸ“‹ è½¬å½•ç»“æœ")

        # è½¬å½•ç»“æœè¡¨æ ¼
        result_table = gr.DataFrame(
            headers=[
                "åºå·",
                "å¼€å§‹æ—¶é—´",
                "ç»“æŸæ—¶é—´",
                "æ—¶é•¿",
                "ç½®ä¿¡åº¦",
                "æ–‡æœ¬å†…å®¹",
            ],
            datatype=["number", "str", "str", "str", "str", "str"],
            interactive=False,
        )

        # SRT å­—å¹•æ–‡ä»¶ä¸‹è½½
        srt_file = gr.File(
            label="ä¸‹è½½ SRT å­—å¹•æ–‡ä»¶", visible=False, interactive=False
        )

        # ç»‘å®šäº‹ä»¶
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        def check_model_status(whisper_type, model_size):
            return self.check_model_download_status(whisper_type, model_size)

        # æ›´æ–°æ¨¡å‹é€‰æ‹©å’Œä¸‹è½½æŒ‰é’®
        def update_models_and_button(whisper_type):
            available_models = self.get_available_models(whisper_type)
            # ä¼˜å…ˆé€‰æ‹© medium æ¨¡å‹ï¼Œå¦‚æœä¸å¯ç”¨åˆ™é€‰æ‹©ç¬¬ä¸€ä¸ª
            default_model = None
            if available_models:
                if "medium" in available_models:
                    default_model = "medium"
                else:
                    default_model = available_models[0]

            if default_model:
                _, btn = self.check_model_download_status(
                    whisper_type, default_model
                )
            else:
                btn = gr.Button(visible=False)
            return (
                gr.Dropdown(choices=available_models, value=default_model),
                btn,
            )

        whisper_type.change(
            fn=update_models_and_button,
            inputs=[whisper_type],
            outputs=[model_size, download_model_btn],
        )

        def check_download_button(whisper_type, model_size):
            _, btn = self.check_model_download_status(whisper_type, model_size)
            return btn

        model_size.change(
            fn=check_download_button,
            inputs=[whisper_type, model_size],
            outputs=[download_model_btn],
        )

        # ä¸‹è½½æ¨¡å‹ - åˆ†ä¸¤æ­¥ï¼šå…ˆæ˜¾ç¤ºè¿›åº¦ï¼Œå†æ‰§è¡Œä¸‹è½½
        def start_download_progress(whisper_type, model_size):
            return self.show_download_progress(whisper_type, model_size)

        def complete_download(whisper_type, model_size):
            _, btn = self.download_model(whisper_type, model_size)
            return btn

        # ç‚¹å‡»ä¸‹è½½æŒ‰é’®æ—¶ç«‹å³æ˜¾ç¤ºè¿›åº¦
        download_model_btn.click(
            fn=start_download_progress,
            inputs=[whisper_type, model_size],
            outputs=[download_model_btn],
        ).then(
            fn=complete_download,
            inputs=[whisper_type, model_size],
            outputs=[download_model_btn],
        )

        transcribe_btn.click(
            fn=self.process_audio_transcription,
            inputs=[
                audio_input,
                whisper_type,
                model_size,
                is_translate,
                temperature,
                beam_size,
            ],
            outputs=[
                result_table,
                srt_file,
            ],
        )

    def setup_model_management_interface(self):
        """è®¾ç½®æ¨¡å‹ç®¡ç†ç•Œé¢"""
        # å·²ç¼“å­˜æ¨¡å‹åŒºåŸŸ
        gr.Markdown("### ğŸ“¦ å·²ç¼“å­˜æ¨¡å‹")

        # åˆ·æ–°æŒ‰é’®
        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")

        # æ¨¡å‹åˆ—è¡¨
        model_list = gr.DataFrame(
            label="æ¨¡å‹åˆ—è¡¨",
            headers=["æ¨¡å‹åç§°", "ç±»å‹", "å¤§å°(MB)", "ä¸‹è½½æ—¶é—´"],
            datatype=["str", "str", "number", "str"],
        )

        # ç¼“å­˜ç»Ÿè®¡
        cache_stats = gr.JSON(
            label="ğŸ“Š ç¼“å­˜ç»Ÿè®¡",
            value=self.download_manager.get_cache_stats(),
        )

        # ç¼“å­˜ç®¡ç†åŒºåŸŸ
        gr.Markdown("### ğŸ—‘ï¸ ç¼“å­˜ç®¡ç†")

        # æ¸…ç†ç¼“å­˜
        cleanup_size = gr.Slider(
            minimum=1.0,
            maximum=20.0,
            value=10.0,
            step=0.5,
            label="ç›®æ ‡ç¼“å­˜å¤§å° (GB)",
        )

        cleanup_btn = gr.Button("ğŸ§¹ æ¸…ç†ç¼“å­˜", variant="secondary")

        cleanup_result = gr.Textbox(label="æ¸…ç†ç»“æœ", interactive=False)

        # æ‰‹åŠ¨ä¸‹è½½åŒºåŸŸ
        gr.Markdown("### â¬‡ï¸ æ‰‹åŠ¨ä¸‹è½½")

        with gr.Row():
            download_type = gr.Dropdown(
                choices=list(self.available_implementations.keys()),
                label="Whisper ç±»å‹",
            )

            download_model = gr.Textbox(
                label="æ¨¡å‹åç§°",
                placeholder="ä¾‹å¦‚: base, medium, large-v3",
            )

        manual_download_btn = gr.Button("â¬‡ï¸ ä¸‹è½½æ¨¡å‹", variant="primary")

        download_progress = gr.Textbox(label="ä¸‹è½½è¿›åº¦", interactive=False)

        # ç»‘å®šäº‹ä»¶
        refresh_btn.click(
            fn=self.refresh_model_cache_list, outputs=[model_list, cache_stats]
        )

        cleanup_btn.click(
            fn=self.cleanup_model_cache,
            inputs=[cleanup_size],
            outputs=[cleanup_result, model_list, cache_stats],
        )

        manual_download_btn.click(
            fn=self.manual_download_model,
            inputs=[download_type, download_model],
            outputs=[download_progress, model_list],
        )

    def render_system_info(self):
        """åˆ›å»ºç³»ç»Ÿä¿¡æ¯å†…å®¹"""
        # è·å–ç³»ç»Ÿä¿¡æ¯å¹¶è½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®
        system_info = self.get_system_info_data()

        gr.Dataframe(
            value=system_info,
            headers=["é¡¹ç›®", "å€¼"],
            datatype=["str", "str"],
            interactive=False,
            wrap=True,
        )

    def get_available_models(self, whisper_type: str) -> List[str]:
        """è·å–æŒ‡å®šç±»å‹çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            config = ModelFactory.create_default_config(
                whisper_type=whisper_type
            )
            pipeline = ModelFactory.create_pipeline(whisper_type, config)
            return pipeline.get_available_models()
        except Exception as e:
            print(f"è·å–å¯ç”¨æ¨¡å‹å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨ä½œä¸ºåå¤‡
            return ["tiny", "base", "small", "medium", "large-v3"]

    def check_model_download_status(
        self, whisper_type: str, model_size: str
    ) -> tuple:
        """æ£€æŸ¥æ¨¡å‹çŠ¶æ€ï¼ˆä»…æ˜¾ç¤ºæœªä¸‹è½½çš„æ¨¡å‹ï¼‰"""
        try:
            # è·å–å·²ç¼“å­˜çš„æ¨¡å‹
            cached_models = self.download_manager.list_cached_models()

            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
            model_exists = any(
                model["model_name"] == model_size
                and model["whisper_type"] == whisper_type
                for model in cached_models
            )

            if model_exists:
                # å·²ä¸‹è½½çš„æ¨¡å‹ä¸æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
                return ("", gr.Button(visible=False))
            else:
                return (
                    f"âš ï¸ æ¨¡å‹ {model_size} ({whisper_type}) æœªä¸‹è½½ï¼Œéœ€è¦ä¸‹è½½åä½¿ç”¨",
                    gr.Button(
                        "ğŸ“¥ ä¸‹è½½æ¨¡å‹",
                        visible=True,
                        variant="primary",
                        interactive=True,
                    ),
                )
        except Exception as e:
            print(f"æ£€æŸ¥æ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
            return (f"âŒ æ£€æŸ¥æ¨¡å‹çŠ¶æ€å¤±è´¥: {str(e)}", gr.Button(visible=False))

    def download_model(self, whisper_type: str, model_size: str) -> tuple:
        """ä¸‹è½½æ¨¡å‹"""
        try:
            # ä¸‹è½½æ¨¡å‹
            model_path = self.download_manager.download_whisper_model(
                model_size, whisper_type
            )

            return (
                f"âœ… æ¨¡å‹ {model_size} ({whisper_type}) ä¸‹è½½å®Œæˆ: {model_path}",
                gr.Button(visible=False),
            )
        except Exception as e:
            error_msg = f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}"
            print(error_msg)
            return (
                error_msg,
                gr.Button(
                    "ğŸ“¥ é‡è¯•ä¸‹è½½",
                    visible=True,
                    variant="primary",
                    interactive=True,
                ),
            )

    def show_download_progress(
        self, whisper_type: str, model_size: str
    ) -> gr.Button:
        """å¼€å§‹ä¸‹è½½å¹¶æ˜¾ç¤ºè¿›åº¦çŠ¶æ€"""
        # ç«‹å³è¿”å›è¿›åº¦çŠ¶æ€æŒ‰é’®
        return gr.Button(
            f"â³ æ­£åœ¨ä¸‹è½½ {model_size}...",
            variant="secondary",
            visible=True,
            interactive=False,
        )

    def update_model_dropdown(self, whisper_type: str) -> gr.Dropdown:
        """æ›´æ–°æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰èœå•"""
        try:
            # è·å–å·²ç¼“å­˜çš„æ¨¡å‹
            cached_models = self.download_manager.list_cached_models()

            # ç­›é€‰å‡ºå¯¹åº”ç±»å‹çš„æ¨¡å‹
            available_models = [
                model["model_name"]
                for model in cached_models
                if model["whisper_type"] == whisper_type
            ]

            if not available_models:
                # å¦‚æœæ²¡æœ‰å·²ä¸‹è½½çš„æ¨¡å‹ï¼Œæ˜¾ç¤ºæç¤º
                return gr.Dropdown(
                    choices=["è¯·å…ˆåœ¨æ¨¡å‹ç®¡ç†ä¸­ä¸‹è½½æ¨¡å‹"],
                    value="è¯·å…ˆåœ¨æ¨¡å‹ç®¡ç†ä¸­ä¸‹è½½æ¨¡å‹",
                    interactive=False,
                )

            return gr.Dropdown(
                choices=available_models,
                value=available_models[0],
                interactive=True,
            )
        except Exception as e:
            print(f"æ›´æ–°æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return gr.Dropdown(
                choices=["è¯·å…ˆåœ¨æ¨¡å‹ç®¡ç†ä¸­ä¸‹è½½æ¨¡å‹"],
                value="è¯·å…ˆåœ¨æ¨¡å‹ç®¡ç†ä¸­ä¸‹è½½æ¨¡å‹",
                interactive=False,
            )

    def process_audio_transcription(
        self,
        audio_file: str,
        whisper_type: str,
        model_size: str,
        is_translate: bool,
        temperature: float,
        beam_size: int,
    ) -> Tuple[List[List], gr.File]:
        """è½¬å½•éŸ³é¢‘"""
        if not audio_file:
            return (
                [],
                gr.File(visible=False),
            )

        # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
        try:
            cached_models = self.download_manager.list_cached_models()
            model_exists = any(
                model["model_name"] == model_size
                and model["whisper_type"] == whisper_type
                for model in cached_models
            )

            if not model_exists:
                print(
                    f"æ¨¡å‹ {model_size} ({whisper_type}) æœªä¸‹è½½ï¼Œå¼€å§‹è‡ªåŠ¨ä¸‹è½½..."
                )
                self.download_manager.download_whisper_model(
                    model_size, whisper_type
                )
                print(f"æ¨¡å‹ {model_size} ({whisper_type}) ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å¤±è´¥: {e}")
            return (
                [],
                gr.File(visible=False),
            )

        try:
            # æ›´æ–°è¿›åº¦
            progress_msg = "ğŸ”„ åˆå§‹åŒ–æ¨¡å‹..."

            # åˆ›å»ºé…ç½®
            config = ModelConfig(
                whisper_type=whisper_type,
                model_dir="models",
                auto_download=True,
            )

            # åˆ›å»ºç®¡é“
            self.current_pipeline = ModelFactory.create_pipeline(
                whisper_type, config
            )

            # åˆ›å»ºå‚æ•°
            params = WhisperParams(
                model_size=model_size,  # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹
                language=None,  # None è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹è¯­è¨€
                is_translate=is_translate,
                temperature=temperature,
                beam_size=int(beam_size),
            )

            # è¿›åº¦å›è°ƒ
            def progress_callback(progress: float, message: str):
                nonlocal progress_msg
                progress_msg = f"ğŸ“ˆ {message} ({progress * 100:.1f}%)"

            # æ‰§è¡Œè½¬å½•
            result = self.current_pipeline.transcribe(
                audio_file, params, progress_callback=progress_callback
            )

            # æ ¼å¼åŒ–ç»“æœ
            table_data = []
            for i, segment in enumerate(result.segments):
                if segment.start is not None and segment.end is not None:
                    duration = segment.end - segment.start
                    confidence = (
                        f"{(1 - (segment.no_speech_prob or 0)) * 100:.1f}%"
                    )
                    text = segment.text.strip() if segment.text else ""

                    table_data.append(
                        [
                            i + 1,
                            self.current_pipeline.format_timestamp(
                                segment.start
                            ),
                            self.current_pipeline.format_timestamp(
                                segment.end
                            ),
                            f"{duration:.1f}s",
                            confidence,
                            text,
                        ]
                    )

            # ç”Ÿæˆ SRT æ–‡ä»¶
            srt_content = self.current_pipeline.generate_srt(result.segments)

            # ç”Ÿæˆæœ‰æ„ä¹‰çš„æ–‡ä»¶å
            import datetime

            audio_name = os.path.splitext(os.path.basename(audio_file))[0]
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{audio_name}_{timestamp}.srt"

            # åˆ›å»ºä¸´æ—¶ SRT æ–‡ä»¶
            temp_dir = tempfile.gettempdir()
            temp_path = os.path.join(temp_dir, filename)

            with open(temp_path, "w", encoding="utf-8") as f:
                f.write(srt_content)

            return (
                table_data,
                gr.File(value=temp_path, visible=True),
            )

        except Exception as e:
            error_msg = f"âŒ è½¬å½•å¤±è´¥: {str(e)}"
            print(error_msg)  # è¾“å‡ºåˆ°æ§åˆ¶å°
            return (
                [],
                gr.File(visible=False),
            )

    def generate_subtitle_file(
        self, table_data: List[List], audio_file: str
    ) -> gr.File:
        """ç”Ÿæˆå­—å¹•æ–‡ä»¶"""
        # Check if table_data is empty (handle both list and DataFrame)
        is_empty = (
            not table_data
            if isinstance(table_data, list)
            else table_data is None or len(table_data) == 0
        )
        if is_empty or not self.current_pipeline:
            return gr.File(visible=False)

        try:
            # é‡å»º segments
            segments = []
            for row in table_data:
                try:
                    # éªŒè¯æ•°æ®æ ¼å¼
                    if len(row) < 6:
                        continue

                    # è§£ææ—¶é—´æˆ³ï¼Œæ·»åŠ é”™è¯¯å¤„ç†
                    start_parts = str(row[1]).split(":")
                    if len(start_parts) != 3:
                        continue

                    start_seconds = (
                        int(start_parts[0]) * 3600
                        + int(start_parts[1]) * 60
                        + float(start_parts[2].replace(",", "."))
                    )

                    end_parts = str(row[2]).split(":")
                    if len(end_parts) != 3:
                        continue

                    end_seconds = (
                        int(end_parts[0]) * 3600
                        + int(end_parts[1]) * 60
                        + float(end_parts[2].replace(",", "."))
                    )

                    from modules.data_classes import Segment

                    segment = Segment(
                        id=int(row[0]) if str(row[0]).isdigit() else 0,
                        start=start_seconds,
                        end=end_seconds,
                        text=str(row[5]) if len(row) > 5 else "",
                    )
                    segments.append(segment)

                except (ValueError, IndexError) as e:
                    # è·³è¿‡æ— æ³•è§£æçš„è¡Œ
                    print(f"è·³è¿‡æ— æ•ˆæ•°æ®è¡Œ: {row}, é”™è¯¯: {e}")
                    continue

            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ segments
            if not segments:
                print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„å­—å¹•æ®µè½æ•°æ®")
                return gr.File(visible=False)

            print(f"æˆåŠŸè§£æ {len(segments)} ä¸ªå­—å¹•æ®µè½")

            # ç”Ÿæˆ SRT å†…å®¹
            srt_content = self.current_pipeline.generate_srt(segments)

            if not srt_content or not srt_content.strip():
                print("è­¦å‘Š: ç”Ÿæˆçš„ SRT å†…å®¹ä¸ºç©º")
                return gr.File(visible=False)

            print(f"ç”Ÿæˆçš„ SRT å†…å®¹é•¿åº¦: {len(srt_content)} å­—ç¬¦")

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(
                mode="w", suffix=".srt", delete=False, encoding="utf-8"
            ) as f:
                f.write(srt_content)
                temp_path = f.name

            return gr.File(value=temp_path, visible=True)

        except Exception as e:
            print(f"ç”Ÿæˆå­—å¹•æ–‡ä»¶å¤±è´¥: {e}")
            return gr.File(visible=False)

    def refresh_model_cache_list(self) -> Tuple[List[List], dict]:
        """åˆ·æ–°æ¨¡å‹åˆ—è¡¨"""
        try:
            models = self.download_manager.list_cached_models()
            table_data = []

            for model in models:
                table_data.append(
                    [
                        model["model_name"],
                        model["whisper_type"],
                        round(model["size_mb"], 1),
                        model["download_time"][:19].replace("T", " "),
                    ]
                )

            stats = self.download_manager.get_cache_stats()
            return table_data, stats

        except Exception as e:
            print(f"åˆ·æ–°æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return [], {"downloads": {}, "last_cleanup": None, "total_size": 0}

    def cleanup_model_cache(
        self, target_size: float
    ) -> Tuple[str, List[List], dict]:
        """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
        try:
            result = self.download_manager.cleanup_cache(target_size)

            freed_mb = result["freed_bytes"] / (1024 * 1024)
            message = (
                f"æ¸…ç†å®Œæˆï¼šåˆ é™¤äº† {result['removed_count']} ä¸ªæ¨¡å‹ï¼Œ"
                f"é‡Šæ”¾äº† {freed_mb:.1f} MB ç©ºé—´"
            )

            # åˆ·æ–°åˆ—è¡¨
            models, stats = self.refresh_model_cache_list()

            return message, models, stats

        except Exception as e:
            return f"æ¸…ç†å¤±è´¥: {str(e)}", [], {}

    def manual_download_model(
        self, whisper_type: str, model_name: str
    ) -> Tuple[str, List[List]]:
        """æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹"""
        if not whisper_type or not model_name:
            return "è¯·é€‰æ‹© Whisper ç±»å‹å’Œæ¨¡å‹åç§°", []

        try:
            progress_msg = "å¼€å§‹ä¸‹è½½..."

            def progress_callback(progress: float, message: str):
                nonlocal progress_msg
                progress_msg = f"{message} ({progress * 100:.1f}%)"

            # ä¸‹è½½æ¨¡å‹
            model_path = self.download_manager.download_whisper_model(
                model_name, whisper_type, progress_callback
            )

            # åˆ·æ–°åˆ—è¡¨
            models, _ = self.refresh_model_cache_list()

            return f"âœ… ä¸‹è½½å®Œæˆ: {model_path}", models

        except Exception as e:
            return f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}", []

    def get_system_info_data(self) -> List[List[str]]:
        """è·å–ç³»ç»Ÿä¿¡æ¯è¡¨æ ¼æ•°æ®"""
        import platform

        try:
            import torch

            cuda_available = torch.cuda.is_available()
            cuda_version = torch.version.cuda if cuda_available else None
        except ImportError:
            cuda_available = False
            cuda_version = None

        try:
            import psutil

            cpu_count = psutil.cpu_count()
            memory_gb = round(psutil.virtual_memory().total / (1024**3), 1)
        except ImportError:
            cpu_count = 0
            memory_gb = 0

        # è½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®ï¼Œä½¿ç”¨ä¸­æ–‡å­—æ®µå
        table_data = [
            ["æ“ä½œç³»ç»Ÿ", platform.platform()],
            ["Python ç‰ˆæœ¬", platform.python_version()],
            ["CPU æ ¸å¿ƒæ•°", str(cpu_count)],
            ["å†…å­˜å¤§å°", f"{memory_gb} GB"],
            ["CUDA æ”¯æŒ", "æ˜¯" if cuda_available else "å¦"],
        ]

        # åªæœ‰åœ¨ CUDA å¯ç”¨æ—¶æ‰æ˜¾ç¤ºç‰ˆæœ¬
        if cuda_available and cuda_version:
            table_data.append(["CUDA ç‰ˆæœ¬", cuda_version])

        return table_data


def main():
    """ä¸»å‡½æ•°"""
    app = VoiceCaptionUI()
    demo = app.create_interface()

    demo.launch(
        server_name="0.0.0.0", server_port=17860, share=False, show_error=False
    )


if __name__ == "__main__":
    main()
